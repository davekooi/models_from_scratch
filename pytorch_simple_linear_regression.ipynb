{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses pytorch to calculate the gradients for a simple linear regression example.\n",
    "\n",
    "It shows how to define parameters that require gradient calculations, calculating the gradient, and update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor([[1],[2],[3]])\n",
    "y = torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define out cost function\n",
    "def mae(preds, acts): return (torch.abs(preds-acts)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 loss 0.5\n",
      "Iter 1 loss 0.49755558371543884\n",
      "Iter 2 loss 0.4932222068309784\n",
      "Iter 3 loss 0.4870000183582306\n",
      "Iter 4 loss 0.4788888692855835\n",
      "Iter 5 loss 0.46888887882232666\n",
      "Iter 6 loss 0.45699992775917053\n",
      "Iter 7 loss 0.44322219491004944\n",
      "Iter 8 loss 0.42755553126335144\n",
      "Iter 9 loss 0.40999993681907654\n",
      "Iter 10 loss 0.3905555009841919\n",
      "Iter 11 loss 0.36922216415405273\n",
      "Iter 12 loss 0.34599995613098145\n",
      "Iter 13 loss 0.32088878750801086\n",
      "Iter 14 loss 0.29388877749443054\n",
      "Iter 15 loss 0.2649998962879181\n",
      "Iter 16 loss 0.2342221736907959\n",
      "Iter 17 loss 0.20155549049377441\n",
      "Iter 18 loss 0.18766669929027557\n",
      "Iter 19 loss 0.18966667354106903\n",
      "Iter 20 loss 0.19155557453632355\n",
      "Iter 21 loss 0.194000244140625\n",
      "Iter 22 loss 0.24300019443035126\n",
      "Iter 23 loss 0.2870001792907715\n",
      "Iter 24 loss 0.32600024342536926\n",
      "Iter 25 loss 0.36000022292137146\n",
      "Iter 26 loss 0.3890000879764557\n",
      "Iter 27 loss 0.41300010681152344\n",
      "Iter 28 loss 0.43200019001960754\n",
      "Iter 29 loss 0.44600018858909607\n",
      "Iter 30 loss 0.4550003111362457\n",
      "Iter 31 loss 0.45900025963783264\n",
      "Iter 32 loss 0.4580002725124359\n",
      "Iter 33 loss 0.452000230550766\n",
      "Iter 34 loss 0.44100022315979004\n",
      "Iter 35 loss 0.42500022053718567\n",
      "Iter 36 loss 0.40400028228759766\n",
      "Iter 37 loss 0.37800025939941406\n",
      "Iter 38 loss 0.34700021147727966\n",
      "Iter 39 loss 0.3110003173351288\n",
      "Iter 40 loss 0.2700002193450928\n",
      "Iter 41 loss 0.22400029003620148\n",
      "Iter 42 loss 0.17300033569335938\n",
      "Iter 43 loss 0.14811110496520996\n",
      "Iter 44 loss 0.1424444168806076\n",
      "Iter 45 loss 0.1366666555404663\n",
      "Iter 46 loss 0.16611091792583466\n",
      "Iter 47 loss 0.19633309543132782\n",
      "Iter 48 loss 0.22466643154621124\n",
      "Iter 49 loss 0.25111091136932373\n",
      "Iter 50 loss 0.2756664752960205\n",
      "Iter 51 loss 0.298333078622818\n",
      "Iter 52 loss 0.33199968934059143\n",
      "Iter 53 loss 0.3659997880458832\n",
      "Iter 54 loss 0.3949996531009674\n",
      "Iter 55 loss 0.4189997613430023\n",
      "Iter 56 loss 0.4379996359348297\n",
      "Iter 57 loss 0.45199957489967346\n",
      "Iter 58 loss 0.46099963784217834\n",
      "Iter 59 loss 0.4649996757507324\n",
      "Iter 60 loss 0.4639996588230133\n",
      "Iter 61 loss 0.4579996168613434\n",
      "Iter 62 loss 0.4469996988773346\n",
      "Iter 63 loss 0.43099960684776306\n",
      "Iter 64 loss 0.40999969840049744\n",
      "Iter 65 loss 0.38399967551231384\n",
      "Iter 66 loss 0.3529996871948242\n",
      "Iter 67 loss 0.3169997036457062\n",
      "Iter 68 loss 0.27599969506263733\n",
      "Iter 69 loss 0.22999966144561768\n",
      "Iter 70 loss 0.1801108568906784\n",
      "Iter 71 loss 0.14499974250793457\n",
      "Iter 72 loss 0.10799971967935562\n",
      "Iter 73 loss 0.06911078840494156\n",
      "Iter 74 loss 0.06499997526407242\n",
      "Iter 75 loss 0.11766711622476578\n",
      "Iter 76 loss 0.17533381283283234\n",
      "Iter 77 loss 0.22800040245056152\n",
      "Iter 78 loss 0.2756669521331787\n",
      "Iter 79 loss 0.3183336555957794\n",
      "Iter 80 loss 0.3560004234313965\n",
      "Iter 81 loss 0.38866710662841797\n",
      "Iter 82 loss 0.4163338840007782\n",
      "Iter 83 loss 0.43900036811828613\n",
      "Iter 84 loss 0.4566670358181\n",
      "Iter 85 loss 0.4693336486816406\n",
      "Iter 86 loss 0.47700047492980957\n",
      "Iter 87 loss 0.4796670377254486\n",
      "Iter 88 loss 0.47733378410339355\n",
      "Iter 89 loss 0.470000296831131\n",
      "Iter 90 loss 0.45766711235046387\n",
      "Iter 91 loss 0.44033387303352356\n",
      "Iter 92 loss 0.4180004596710205\n",
      "Iter 93 loss 0.39066699147224426\n",
      "Iter 94 loss 0.3583337366580963\n",
      "Iter 95 loss 0.321000337600708\n",
      "Iter 96 loss 0.2786672115325928\n",
      "Iter 97 loss 0.23133373260498047\n",
      "Iter 98 loss 0.17900045216083527\n",
      "Iter 99 loss 0.12166714668273926\n"
     ]
    }
   ],
   "source": [
    "# Pad with ones so we have something to multiply the bias by\n",
    "X_with_ones = torch.nn.functional.pad(X, (0,1), 'constant', 1)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define the parameters we'd like to track the gradient calulation for\n",
    "weight_and_bias = torch.tensor([0.5, 0.5], requires_grad=True)\n",
    "\n",
    "# Training loop\n",
    "for iter in range(100):\n",
    "  preds = torch.sum(torch.mul(weight_and_bias, X_with_ones), dim=1)\n",
    "  loss = mae(preds, y)\n",
    "  print(f\"Iter {iter} loss {loss}\")\n",
    "\n",
    "  # Calculate the gradients\n",
    "  loss.backward()\n",
    "\n",
    "  # Update the weight and bias terms using the gradient\n",
    "  # torch.no_grad() avoids calculating the gradients for this step\n",
    "  with torch.no_grad():\n",
    "    weight_and_bias -= weight_and_bias.grad * learning_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3870,  1.0057,  2.3983,  3.7910,  5.1837])\n"
     ]
    }
   ],
   "source": [
    "X_test = torch.tensor([[0],[1],[2],[3],[4]])\n",
    "X_test_with_ones = torch.nn.functional.pad(X_test, (0,1), 'constant', 1)\n",
    "\n",
    "with torch.no_grad():\n",
    "  preds = torch.sum(torch.mul(weight_and_bias, X_test_with_ones), dim=1)\n",
    "  print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
