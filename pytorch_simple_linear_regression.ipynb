{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses pytorch to calculate the gradients for a simple linear regression example.\n",
    "\n",
    "It shows how to define parameters that require gradient calculations, calculating the gradient, and update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor([[1],[2],[3]])\n",
    "y = torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define out cost function\n",
    "def mae(preds, acts): return (torch.abs(preds-acts)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 loss 0.5\n",
      "Iter 1 loss 0.47555556893348694\n",
      "Iter 2 loss 0.4566667079925537\n",
      "Iter 3 loss 0.4377778470516205\n",
      "Iter 4 loss 0.41888895630836487\n",
      "Iter 5 loss 0.40000009536743164\n",
      "Iter 6 loss 0.3811112344264984\n",
      "Iter 7 loss 0.3622223436832428\n",
      "Iter 8 loss 0.34333348274230957\n",
      "Iter 9 loss 0.32444462180137634\n",
      "Iter 10 loss 0.3055557310581207\n",
      "Iter 11 loss 0.2866668701171875\n",
      "Iter 12 loss 0.2677780091762543\n",
      "Iter 13 loss 0.24888913333415985\n",
      "Iter 14 loss 0.23000025749206543\n",
      "Iter 15 loss 0.211111381649971\n",
      "Iter 16 loss 0.19222252070903778\n",
      "Iter 17 loss 0.18666665256023407\n",
      "Iter 18 loss 0.18555553257465363\n",
      "Iter 19 loss 0.18444442749023438\n",
      "Iter 20 loss 0.18333332240581512\n",
      "Iter 21 loss 0.18222220242023468\n",
      "Iter 22 loss 0.18111109733581543\n",
      "Iter 23 loss 0.18000030517578125\n",
      "Iter 24 loss 0.18111109733581543\n",
      "Iter 25 loss 0.17999999225139618\n",
      "Iter 26 loss 0.17888887226581573\n",
      "Iter 27 loss 0.17777776718139648\n",
      "Iter 28 loss 0.17666666209697723\n",
      "Iter 29 loss 0.1755555421113968\n",
      "Iter 30 loss 0.17444443702697754\n",
      "Iter 31 loss 0.1733333319425583\n",
      "Iter 32 loss 0.17222221195697784\n",
      "Iter 33 loss 0.17111141979694366\n",
      "Iter 34 loss 0.17222221195697784\n",
      "Iter 35 loss 0.1711111068725586\n",
      "Iter 36 loss 0.17000000178813934\n",
      "Iter 37 loss 0.1688888818025589\n",
      "Iter 38 loss 0.16777777671813965\n",
      "Iter 39 loss 0.1666666716337204\n",
      "Iter 40 loss 0.16555555164813995\n",
      "Iter 41 loss 0.1644444465637207\n",
      "Iter 42 loss 0.16333334147930145\n",
      "Iter 43 loss 0.16222254931926727\n",
      "Iter 44 loss 0.16333334147930145\n",
      "Iter 45 loss 0.162222221493721\n",
      "Iter 46 loss 0.16111111640930176\n",
      "Iter 47 loss 0.1600000113248825\n",
      "Iter 48 loss 0.15888889133930206\n",
      "Iter 49 loss 0.1577777862548828\n",
      "Iter 50 loss 0.15666668117046356\n",
      "Iter 51 loss 0.15555556118488312\n",
      "Iter 52 loss 0.15444445610046387\n",
      "Iter 53 loss 0.1533336639404297\n",
      "Iter 54 loss 0.15444445610046387\n",
      "Iter 55 loss 0.15333335101604462\n",
      "Iter 56 loss 0.15222223103046417\n",
      "Iter 57 loss 0.15111112594604492\n",
      "Iter 58 loss 0.15000002086162567\n",
      "Iter 59 loss 0.14888890087604523\n",
      "Iter 60 loss 0.14777779579162598\n",
      "Iter 61 loss 0.14666669070720673\n",
      "Iter 62 loss 0.14555557072162628\n",
      "Iter 63 loss 0.1444447785615921\n",
      "Iter 64 loss 0.14555557072162628\n",
      "Iter 65 loss 0.14444446563720703\n",
      "Iter 66 loss 0.14333336055278778\n",
      "Iter 67 loss 0.14222224056720734\n",
      "Iter 68 loss 0.14111113548278809\n",
      "Iter 69 loss 0.14000003039836884\n",
      "Iter 70 loss 0.1388889104127884\n",
      "Iter 71 loss 0.13777780532836914\n",
      "Iter 72 loss 0.1366667002439499\n",
      "Iter 73 loss 0.1355559080839157\n",
      "Iter 74 loss 0.1366667002439499\n",
      "Iter 75 loss 0.13555558025836945\n",
      "Iter 76 loss 0.1344444751739502\n",
      "Iter 77 loss 0.13333337008953094\n",
      "Iter 78 loss 0.1322222501039505\n",
      "Iter 79 loss 0.13111114501953125\n",
      "Iter 80 loss 0.130000039935112\n",
      "Iter 81 loss 0.12888891994953156\n",
      "Iter 82 loss 0.1277778148651123\n",
      "Iter 83 loss 0.12666702270507812\n",
      "Iter 84 loss 0.1277778148651123\n",
      "Iter 85 loss 0.12666670978069305\n",
      "Iter 86 loss 0.1255555897951126\n",
      "Iter 87 loss 0.12444448471069336\n",
      "Iter 88 loss 0.12333337217569351\n",
      "Iter 89 loss 0.12222226709127426\n",
      "Iter 90 loss 0.12111115455627441\n",
      "Iter 91 loss 0.12000004202127457\n",
      "Iter 92 loss 0.11888893693685532\n",
      "Iter 93 loss 0.11777814477682114\n",
      "Iter 94 loss 0.11888893693685532\n",
      "Iter 95 loss 0.11777782440185547\n",
      "Iter 96 loss 0.11666671186685562\n",
      "Iter 97 loss 0.11555560678243637\n",
      "Iter 98 loss 0.11444449424743652\n",
      "Iter 99 loss 0.11333338171243668\n",
      "Iter 100 loss 0.11222227662801743\n",
      "Iter 101 loss 0.11111116409301758\n",
      "Iter 102 loss 0.11000005155801773\n",
      "Iter 103 loss 0.10888925939798355\n",
      "Iter 104 loss 0.11000005155801773\n",
      "Iter 105 loss 0.10888894647359848\n",
      "Iter 106 loss 0.10777783393859863\n",
      "Iter 107 loss 0.10666672140359879\n",
      "Iter 108 loss 0.10555561631917953\n",
      "Iter 109 loss 0.10444450378417969\n",
      "Iter 110 loss 0.10333339124917984\n",
      "Iter 111 loss 0.10222228616476059\n",
      "Iter 112 loss 0.10111117362976074\n",
      "Iter 113 loss 0.10000038146972656\n",
      "Iter 114 loss 0.10111117362976074\n",
      "Iter 115 loss 0.1000000610947609\n",
      "Iter 116 loss 0.09888895601034164\n",
      "Iter 117 loss 0.0977778434753418\n",
      "Iter 118 loss 0.09666673094034195\n",
      "Iter 119 loss 0.0955556258559227\n",
      "Iter 120 loss 0.09444451332092285\n",
      "Iter 121 loss 0.093333400785923\n",
      "Iter 122 loss 0.09222229570150375\n",
      "Iter 123 loss 0.09111150354146957\n",
      "Iter 124 loss 0.09222229570150375\n",
      "Iter 125 loss 0.0911111831665039\n",
      "Iter 126 loss 0.09000007063150406\n",
      "Iter 127 loss 0.08888896554708481\n",
      "Iter 128 loss 0.08777785301208496\n",
      "Iter 129 loss 0.08666674047708511\n",
      "Iter 130 loss 0.08555563539266586\n",
      "Iter 131 loss 0.08444452285766602\n",
      "Iter 132 loss 0.08333341032266617\n",
      "Iter 133 loss 0.08222261816263199\n",
      "Iter 134 loss 0.08333341032266617\n",
      "Iter 135 loss 0.08222230523824692\n",
      "Iter 136 loss 0.08111119270324707\n",
      "Iter 137 loss 0.08000008016824722\n",
      "Iter 138 loss 0.07888897508382797\n",
      "Iter 139 loss 0.07777786254882812\n",
      "Iter 140 loss 0.07666675001382828\n",
      "Iter 141 loss 0.07555564492940903\n",
      "Iter 142 loss 0.07444453239440918\n",
      "Iter 143 loss 0.073333740234375\n",
      "Iter 144 loss 0.07444453239440918\n",
      "Iter 145 loss 0.07333341985940933\n",
      "Iter 146 loss 0.07222231477499008\n",
      "Iter 147 loss 0.07111120223999023\n",
      "Iter 148 loss 0.07000008970499039\n",
      "Iter 149 loss 0.06888898462057114\n",
      "Iter 150 loss 0.06777787208557129\n",
      "Iter 151 loss 0.06666675955057144\n",
      "Iter 152 loss 0.06555565446615219\n",
      "Iter 153 loss 0.06444486230611801\n",
      "Iter 154 loss 0.06555565446615219\n",
      "Iter 155 loss 0.06444454193115234\n",
      "Iter 156 loss 0.0633334293961525\n",
      "Iter 157 loss 0.06222232058644295\n",
      "Iter 158 loss 0.0611112117767334\n",
      "Iter 159 loss 0.06000010296702385\n",
      "Iter 160 loss 0.058888990432024\n",
      "Iter 161 loss 0.05777788162231445\n",
      "Iter 162 loss 0.056666772812604904\n",
      "Iter 163 loss 0.055555980652570724\n",
      "Iter 164 loss 0.056666772812604904\n",
      "Iter 165 loss 0.05555566027760506\n",
      "Iter 166 loss 0.05444455146789551\n",
      "Iter 167 loss 0.05333344265818596\n",
      "Iter 168 loss 0.05222233012318611\n",
      "Iter 169 loss 0.05111122131347656\n",
      "Iter 170 loss 0.050000112503767014\n",
      "Iter 171 loss 0.048888999968767166\n",
      "Iter 172 loss 0.04777789115905762\n",
      "Iter 173 loss 0.04666709899902344\n",
      "Iter 174 loss 0.04777789115905762\n",
      "Iter 175 loss 0.04666678234934807\n",
      "Iter 176 loss 0.04555566981434822\n",
      "Iter 177 loss 0.04444456100463867\n",
      "Iter 178 loss 0.04333345219492912\n",
      "Iter 179 loss 0.042222339659929276\n",
      "Iter 180 loss 0.04111123085021973\n",
      "Iter 181 loss 0.04000012204051018\n",
      "Iter 182 loss 0.03888900950551033\n",
      "Iter 183 loss 0.03777821734547615\n",
      "Iter 184 loss 0.03888900950551033\n",
      "Iter 185 loss 0.03777790069580078\n",
      "Iter 186 loss 0.03666679188609123\n",
      "Iter 187 loss 0.035555679351091385\n",
      "Iter 188 loss 0.034444570541381836\n",
      "Iter 189 loss 0.03333346173167229\n",
      "Iter 190 loss 0.03222234919667244\n",
      "Iter 191 loss 0.03111124038696289\n",
      "Iter 192 loss 0.030000129714608192\n",
      "Iter 193 loss 0.028889337554574013\n",
      "Iter 194 loss 0.030000129714608192\n",
      "Iter 195 loss 0.028889020904898643\n",
      "Iter 196 loss 0.027777910232543945\n",
      "Iter 197 loss 0.026666799560189247\n",
      "Iter 198 loss 0.025555690750479698\n",
      "Iter 199 loss 0.024444580078125\n",
      "Iter 200 loss 0.023333469405770302\n",
      "Iter 201 loss 0.022222360596060753\n",
      "Iter 202 loss 0.021111249923706055\n",
      "Iter 203 loss 0.020000457763671875\n",
      "Iter 204 loss 0.029999494552612305\n",
      "Iter 205 loss 0.03111155889928341\n",
      "Iter 206 loss 0.018889030441641808\n",
      "Iter 207 loss 0.01777791976928711\n",
      "Iter 208 loss 0.01666680909693241\n",
      "Iter 209 loss 0.015556017868220806\n",
      "Iter 210 loss 0.029999494552612305\n",
      "Iter 211 loss 0.026667118072509766\n",
      "Iter 212 loss 0.014444589614868164\n",
      "Iter 213 loss 0.01333347987383604\n",
      "Iter 214 loss 0.012222369201481342\n",
      "Iter 215 loss 0.011111577041447163\n",
      "Iter 216 loss 0.029999494552612305\n",
      "Iter 217 loss 0.022222677245736122\n",
      "Iter 218 loss 0.010000149719417095\n",
      "Iter 219 loss 0.008889039047062397\n",
      "Iter 220 loss 0.0077779293060302734\n",
      "Iter 221 loss 0.006667137145996094\n",
      "Iter 222 loss 0.029999494552612305\n",
      "Iter 223 loss 0.020000478252768517\n",
      "Iter 224 loss 0.029999494552612305\n",
      "Iter 225 loss 0.020000478252768517\n",
      "Iter 226 loss 0.029999494552612305\n",
      "Iter 227 loss 0.020000478252768517\n",
      "Iter 228 loss 0.029999494552612305\n",
      "Iter 229 loss 0.020000478252768517\n",
      "Iter 230 loss 0.029999494552612305\n",
      "Iter 231 loss 0.020000478252768517\n",
      "Iter 232 loss 0.029999494552612305\n",
      "Iter 233 loss 0.020000478252768517\n",
      "Iter 234 loss 0.029999494552612305\n",
      "Iter 235 loss 0.020000478252768517\n",
      "Iter 236 loss 0.029999494552612305\n",
      "Iter 237 loss 0.020000478252768517\n",
      "Iter 238 loss 0.029999494552612305\n",
      "Iter 239 loss 0.020000478252768517\n",
      "Iter 240 loss 0.029999494552612305\n",
      "Iter 241 loss 0.020000478252768517\n",
      "Iter 242 loss 0.029999494552612305\n",
      "Iter 243 loss 0.020000478252768517\n",
      "Iter 244 loss 0.029999494552612305\n",
      "Iter 245 loss 0.020000478252768517\n",
      "Iter 246 loss 0.029999494552612305\n",
      "Iter 247 loss 0.020000478252768517\n",
      "Iter 248 loss 0.029999494552612305\n",
      "Iter 249 loss 0.020000478252768517\n",
      "Iter 250 loss 0.029999494552612305\n",
      "Iter 251 loss 0.020000478252768517\n",
      "Iter 252 loss 0.029999494552612305\n",
      "Iter 253 loss 0.020000478252768517\n",
      "Iter 254 loss 0.029999494552612305\n",
      "Iter 255 loss 0.020000478252768517\n",
      "Iter 256 loss 0.029999494552612305\n",
      "Iter 257 loss 0.020000478252768517\n",
      "Iter 258 loss 0.029999494552612305\n",
      "Iter 259 loss 0.020000478252768517\n",
      "Iter 260 loss 0.029999494552612305\n",
      "Iter 261 loss 0.020000478252768517\n",
      "Iter 262 loss 0.029999494552612305\n",
      "Iter 263 loss 0.020000478252768517\n",
      "Iter 264 loss 0.029999494552612305\n",
      "Iter 265 loss 0.020000478252768517\n",
      "Iter 266 loss 0.029999494552612305\n",
      "Iter 267 loss 0.020000478252768517\n",
      "Iter 268 loss 0.029999494552612305\n",
      "Iter 269 loss 0.020000478252768517\n",
      "Iter 270 loss 0.029999494552612305\n",
      "Iter 271 loss 0.020000478252768517\n",
      "Iter 272 loss 0.029999494552612305\n",
      "Iter 273 loss 0.020000478252768517\n",
      "Iter 274 loss 0.029999494552612305\n",
      "Iter 275 loss 0.020000478252768517\n",
      "Iter 276 loss 0.029999494552612305\n",
      "Iter 277 loss 0.020000478252768517\n",
      "Iter 278 loss 0.029999494552612305\n",
      "Iter 279 loss 0.020000478252768517\n",
      "Iter 280 loss 0.029999494552612305\n",
      "Iter 281 loss 0.020000478252768517\n",
      "Iter 282 loss 0.029999494552612305\n",
      "Iter 283 loss 0.020000478252768517\n",
      "Iter 284 loss 0.029999494552612305\n",
      "Iter 285 loss 0.020000478252768517\n",
      "Iter 286 loss 0.029999494552612305\n",
      "Iter 287 loss 0.020000478252768517\n",
      "Iter 288 loss 0.029999494552612305\n",
      "Iter 289 loss 0.020000478252768517\n",
      "Iter 290 loss 0.029999494552612305\n",
      "Iter 291 loss 0.020000478252768517\n",
      "Iter 292 loss 0.029999494552612305\n",
      "Iter 293 loss 0.020000478252768517\n",
      "Iter 294 loss 0.029999494552612305\n",
      "Iter 295 loss 0.020000478252768517\n",
      "Iter 296 loss 0.029999494552612305\n",
      "Iter 297 loss 0.020000478252768517\n",
      "Iter 298 loss 0.029999494552612305\n",
      "Iter 299 loss 0.020000478252768517\n",
      "Iter 300 loss 0.029999494552612305\n",
      "Iter 301 loss 0.020000478252768517\n",
      "Iter 302 loss 0.029999494552612305\n",
      "Iter 303 loss 0.020000478252768517\n",
      "Iter 304 loss 0.029999494552612305\n",
      "Iter 305 loss 0.020000478252768517\n",
      "Iter 306 loss 0.029999494552612305\n",
      "Iter 307 loss 0.020000478252768517\n",
      "Iter 308 loss 0.029999494552612305\n",
      "Iter 309 loss 0.020000478252768517\n",
      "Iter 310 loss 0.029999494552612305\n",
      "Iter 311 loss 0.020000478252768517\n",
      "Iter 312 loss 0.029999494552612305\n",
      "Iter 313 loss 0.020000478252768517\n",
      "Iter 314 loss 0.029999494552612305\n",
      "Iter 315 loss 0.020000478252768517\n",
      "Iter 316 loss 0.029999494552612305\n",
      "Iter 317 loss 0.020000478252768517\n",
      "Iter 318 loss 0.029999494552612305\n",
      "Iter 319 loss 0.020000478252768517\n",
      "Iter 320 loss 0.029999494552612305\n",
      "Iter 321 loss 0.020000478252768517\n",
      "Iter 322 loss 0.029999494552612305\n",
      "Iter 323 loss 0.020000478252768517\n",
      "Iter 324 loss 0.029999494552612305\n",
      "Iter 325 loss 0.020000478252768517\n",
      "Iter 326 loss 0.029999494552612305\n",
      "Iter 327 loss 0.020000478252768517\n",
      "Iter 328 loss 0.029999494552612305\n",
      "Iter 329 loss 0.020000478252768517\n",
      "Iter 330 loss 0.029999494552612305\n",
      "Iter 331 loss 0.020000478252768517\n",
      "Iter 332 loss 0.029999494552612305\n",
      "Iter 333 loss 0.020000478252768517\n",
      "Iter 334 loss 0.029999494552612305\n",
      "Iter 335 loss 0.020000478252768517\n",
      "Iter 336 loss 0.029999494552612305\n",
      "Iter 337 loss 0.020000478252768517\n",
      "Iter 338 loss 0.029999494552612305\n",
      "Iter 339 loss 0.020000478252768517\n",
      "Iter 340 loss 0.029999494552612305\n",
      "Iter 341 loss 0.020000478252768517\n",
      "Iter 342 loss 0.029999494552612305\n",
      "Iter 343 loss 0.020000478252768517\n",
      "Iter 344 loss 0.029999494552612305\n",
      "Iter 345 loss 0.020000478252768517\n",
      "Iter 346 loss 0.029999494552612305\n",
      "Iter 347 loss 0.020000478252768517\n",
      "Iter 348 loss 0.029999494552612305\n",
      "Iter 349 loss 0.020000478252768517\n",
      "Iter 350 loss 0.029999494552612305\n",
      "Iter 351 loss 0.020000478252768517\n",
      "Iter 352 loss 0.029999494552612305\n",
      "Iter 353 loss 0.020000478252768517\n",
      "Iter 354 loss 0.029999494552612305\n",
      "Iter 355 loss 0.020000478252768517\n",
      "Iter 356 loss 0.029999494552612305\n",
      "Iter 357 loss 0.020000478252768517\n",
      "Iter 358 loss 0.029999494552612305\n",
      "Iter 359 loss 0.020000478252768517\n",
      "Iter 360 loss 0.029999494552612305\n",
      "Iter 361 loss 0.020000478252768517\n",
      "Iter 362 loss 0.029999494552612305\n",
      "Iter 363 loss 0.020000478252768517\n",
      "Iter 364 loss 0.029999494552612305\n",
      "Iter 365 loss 0.020000478252768517\n",
      "Iter 366 loss 0.029999494552612305\n",
      "Iter 367 loss 0.020000478252768517\n",
      "Iter 368 loss 0.029999494552612305\n",
      "Iter 369 loss 0.020000478252768517\n",
      "Iter 370 loss 0.029999494552612305\n",
      "Iter 371 loss 0.020000478252768517\n",
      "Iter 372 loss 0.029999494552612305\n",
      "Iter 373 loss 0.020000478252768517\n",
      "Iter 374 loss 0.029999494552612305\n",
      "Iter 375 loss 0.020000478252768517\n",
      "Iter 376 loss 0.029999494552612305\n",
      "Iter 377 loss 0.020000478252768517\n",
      "Iter 378 loss 0.029999494552612305\n",
      "Iter 379 loss 0.020000478252768517\n",
      "Iter 380 loss 0.029999494552612305\n",
      "Iter 381 loss 0.020000478252768517\n",
      "Iter 382 loss 0.029999494552612305\n",
      "Iter 383 loss 0.020000478252768517\n",
      "Iter 384 loss 0.029999494552612305\n",
      "Iter 385 loss 0.020000478252768517\n",
      "Iter 386 loss 0.029999494552612305\n",
      "Iter 387 loss 0.020000478252768517\n",
      "Iter 388 loss 0.029999494552612305\n",
      "Iter 389 loss 0.020000478252768517\n",
      "Iter 390 loss 0.029999494552612305\n",
      "Iter 391 loss 0.020000478252768517\n",
      "Iter 392 loss 0.029999494552612305\n",
      "Iter 393 loss 0.020000478252768517\n",
      "Iter 394 loss 0.029999494552612305\n",
      "Iter 395 loss 0.020000478252768517\n",
      "Iter 396 loss 0.029999494552612305\n",
      "Iter 397 loss 0.020000478252768517\n",
      "Iter 398 loss 0.029999494552612305\n",
      "Iter 399 loss 0.020000478252768517\n",
      "Iter 400 loss 0.029999494552612305\n",
      "Iter 401 loss 0.020000478252768517\n",
      "Iter 402 loss 0.029999494552612305\n",
      "Iter 403 loss 0.020000478252768517\n",
      "Iter 404 loss 0.029999494552612305\n",
      "Iter 405 loss 0.020000478252768517\n",
      "Iter 406 loss 0.029999494552612305\n",
      "Iter 407 loss 0.020000478252768517\n",
      "Iter 408 loss 0.029999494552612305\n",
      "Iter 409 loss 0.020000478252768517\n",
      "Iter 410 loss 0.029999494552612305\n",
      "Iter 411 loss 0.020000478252768517\n",
      "Iter 412 loss 0.029999494552612305\n",
      "Iter 413 loss 0.020000478252768517\n",
      "Iter 414 loss 0.029999494552612305\n",
      "Iter 415 loss 0.020000478252768517\n",
      "Iter 416 loss 0.029999494552612305\n",
      "Iter 417 loss 0.020000478252768517\n",
      "Iter 418 loss 0.029999494552612305\n",
      "Iter 419 loss 0.020000478252768517\n",
      "Iter 420 loss 0.029999494552612305\n",
      "Iter 421 loss 0.020000478252768517\n",
      "Iter 422 loss 0.029999494552612305\n",
      "Iter 423 loss 0.020000478252768517\n",
      "Iter 424 loss 0.029999494552612305\n",
      "Iter 425 loss 0.020000478252768517\n",
      "Iter 426 loss 0.029999494552612305\n",
      "Iter 427 loss 0.020000478252768517\n",
      "Iter 428 loss 0.029999494552612305\n",
      "Iter 429 loss 0.020000478252768517\n",
      "Iter 430 loss 0.029999494552612305\n",
      "Iter 431 loss 0.020000478252768517\n",
      "Iter 432 loss 0.029999494552612305\n",
      "Iter 433 loss 0.020000478252768517\n",
      "Iter 434 loss 0.029999494552612305\n",
      "Iter 435 loss 0.020000478252768517\n",
      "Iter 436 loss 0.029999494552612305\n",
      "Iter 437 loss 0.020000478252768517\n",
      "Iter 438 loss 0.029999494552612305\n",
      "Iter 439 loss 0.020000478252768517\n",
      "Iter 440 loss 0.029999494552612305\n",
      "Iter 441 loss 0.020000478252768517\n",
      "Iter 442 loss 0.029999494552612305\n",
      "Iter 443 loss 0.020000478252768517\n",
      "Iter 444 loss 0.029999494552612305\n",
      "Iter 445 loss 0.020000478252768517\n",
      "Iter 446 loss 0.029999494552612305\n",
      "Iter 447 loss 0.020000478252768517\n",
      "Iter 448 loss 0.029999494552612305\n",
      "Iter 449 loss 0.020000478252768517\n",
      "Iter 450 loss 0.029999494552612305\n",
      "Iter 451 loss 0.020000478252768517\n",
      "Iter 452 loss 0.029999494552612305\n",
      "Iter 453 loss 0.020000478252768517\n",
      "Iter 454 loss 0.029999494552612305\n",
      "Iter 455 loss 0.020000478252768517\n",
      "Iter 456 loss 0.029999494552612305\n",
      "Iter 457 loss 0.020000478252768517\n",
      "Iter 458 loss 0.029999494552612305\n",
      "Iter 459 loss 0.020000478252768517\n",
      "Iter 460 loss 0.029999494552612305\n",
      "Iter 461 loss 0.020000478252768517\n",
      "Iter 462 loss 0.029999494552612305\n",
      "Iter 463 loss 0.020000478252768517\n",
      "Iter 464 loss 0.029999494552612305\n",
      "Iter 465 loss 0.020000478252768517\n",
      "Iter 466 loss 0.029999494552612305\n",
      "Iter 467 loss 0.020000478252768517\n",
      "Iter 468 loss 0.029999494552612305\n",
      "Iter 469 loss 0.020000478252768517\n",
      "Iter 470 loss 0.029999494552612305\n",
      "Iter 471 loss 0.020000478252768517\n",
      "Iter 472 loss 0.029999494552612305\n",
      "Iter 473 loss 0.020000478252768517\n",
      "Iter 474 loss 0.029999494552612305\n",
      "Iter 475 loss 0.020000478252768517\n",
      "Iter 476 loss 0.029999494552612305\n",
      "Iter 477 loss 0.020000478252768517\n",
      "Iter 478 loss 0.029999494552612305\n",
      "Iter 479 loss 0.020000478252768517\n",
      "Iter 480 loss 0.029999494552612305\n",
      "Iter 481 loss 0.020000478252768517\n",
      "Iter 482 loss 0.029999494552612305\n",
      "Iter 483 loss 0.020000478252768517\n",
      "Iter 484 loss 0.029999494552612305\n",
      "Iter 485 loss 0.020000478252768517\n",
      "Iter 486 loss 0.029999494552612305\n",
      "Iter 487 loss 0.020000478252768517\n",
      "Iter 488 loss 0.029999494552612305\n",
      "Iter 489 loss 0.020000478252768517\n",
      "Iter 490 loss 0.029999494552612305\n",
      "Iter 491 loss 0.020000478252768517\n",
      "Iter 492 loss 0.029999494552612305\n",
      "Iter 493 loss 0.020000478252768517\n",
      "Iter 494 loss 0.029999494552612305\n",
      "Iter 495 loss 0.020000478252768517\n",
      "Iter 496 loss 0.029999494552612305\n",
      "Iter 497 loss 0.020000478252768517\n",
      "Iter 498 loss 0.029999494552612305\n",
      "Iter 499 loss 0.020000478252768517\n"
     ]
    }
   ],
   "source": [
    "# Pad with ones so we have something to multiply the bias by\n",
    "X_with_ones = torch.nn.functional.pad(X, (0,1), 'constant', 1)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Define the parameters we'd like to track the gradient calulation for\n",
    "weight_and_bias = torch.tensor([0.5, 0.5], requires_grad=True)\n",
    "\n",
    "# Training loop\n",
    "for iter in range(500):\n",
    "  preds = torch.sum(torch.mul(weight_and_bias, X_with_ones), dim=1)\n",
    "  loss = mae(preds, y)\n",
    "  print(f\"Iter {iter} loss {loss}\")\n",
    "\n",
    "  # Calculate the gradients\n",
    "  loss.backward()\n",
    "\n",
    "  # Update the weight and bias terms using the gradient\n",
    "  # torch.no_grad() avoids calculating the gradients for this step\n",
    "  with torch.no_grad():\n",
    "    weight_and_bias.sub_(weight_and_bias.grad * learning_rate)\n",
    "    weight_and_bias.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3870,  1.0057,  2.3983,  3.7910,  5.1837])\n"
     ]
    }
   ],
   "source": [
    "X_test = torch.tensor([[0],[1],[2],[3],[4]])\n",
    "X_test_with_ones = torch.nn.functional.pad(X_test, (0,1), 'constant', 1)\n",
    "\n",
    "with torch.no_grad():\n",
    "  preds = torch.sum(torch.mul(weight_and_bias, X_test_with_ones), dim=1)\n",
    "  print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
